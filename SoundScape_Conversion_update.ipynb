{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gylxZkMmAjga"
      },
      "source": [
        "##Introduction\n",
        "\n",
        "This notebook converts a CSV file into a netCDF format using MANTA converter tools. It retains the original structure of the provided scripts and uses a settings file to configure parameters. Users can upload a CSV file, and the converted netCDF file will be available for download.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfhnQPhWAjgc"
      },
      "source": [
        "##Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chktE8xnAjgd",
        "outputId": "13be0ca0-492a-407e-84b1-aaab121fa730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting netCDF4\n",
            "  Downloading netCDF4-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting cftime (from netCDF4)\n",
            "  Downloading cftime-1.6.4.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from netCDF4) (2024.8.30)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from netCDF4) (1.26.4)\n",
            "Downloading netCDF4-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cftime-1.6.4.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cftime, netCDF4\n",
            "Successfully installed cftime-1.6.4.post1 netCDF4-1.7.2\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (2024.10.0)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from xarray) (1.26.4)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from xarray) (24.2)\n",
            "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.10/dist-packages (from xarray) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->xarray) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->xarray) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->xarray) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1->xarray) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install netCDF4\n",
        "!pip install xarray\n",
        "!pip install numpy\n",
        "!pip install pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "Gm8xCAWJEUQd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrMSD2vkAjgd",
        "outputId": "b513a474-6d79-4ec8-c35f-92d16545e18a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the base directory path: /content/drive/My Drive/SoundScape Conversion files\n",
            "\n",
            "Input files located successfully:\n",
            "CSV file: /content/drive/My Drive/SoundScape Conversion files/csv/NRS01_H5R6B.1.5000_20180831_DAILY_MILLIDEC_MinRes_v3.csv\n",
            "Metadata file: /content/drive/My Drive/SoundScape Conversion files/metadata/NRS_01_20180820-20200824_HMD_v3.json\n",
            "Calibration file: /content/drive/My Drive/SoundScape Conversion files/calibration/NRS01_1820_MANTA_Metadata_v3.xlsx\n",
            "Settings file: /content/drive/My Drive/SoundScape Conversion files/settings file/nrs-hmd.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define paths to your files\n",
        "# csv_file = r'F:/python/my projects/netCDF/data/NRS01_H5R6B.1.5000_20180831_DAILY_MILLIDEC_MinRes_v3.csv'\n",
        "\n",
        "# metadata_file = r'F:/python/my projects/netCDF/metadata/NRS_01_20180820-20200824_HMD_v3.json'\n",
        "\n",
        "# cal_file=r\"F:/python/my projects/netCDF/calibration/NRS01_1820_MANTA_Metadata_v3.xlsx\"\n",
        "# data_dir=r\"F:/python/my projects/netCDF/data\"\n",
        "# settings_file=r\"F:/python/my projects/netCDF/settings.json\"\n",
        "\n",
        "\n",
        "# Ask the user for the base directory path\n",
        "base_dir = input(\"Enter the base directory path: \").strip()\n",
        "\n",
        "# Automatically locate the required files in their respective subdirectories\n",
        "csv_file = os.path.join(base_dir, 'csv', 'NRS01_H5R6B.1.5000_20180831_DAILY_MILLIDEC_MinRes_v3.csv')\n",
        "metadata_file = os.path.join(base_dir, 'metadata', 'NRS_01_20180820-20200824_HMD_v3.json')\n",
        "cal_file = os.path.join(base_dir, 'calibration', 'NRS01_1820_MANTA_Metadata_v3.xlsx')\n",
        "settings_file = os.path.join(base_dir, 'settings file', 'nrs-hmd.json')\n",
        "\n",
        "# Print the located file paths\n",
        "print(\"\\nInput files located successfully:\")\n",
        "print(f\"CSV file: {csv_file}\")\n",
        "print(f\"Metadata file: {metadata_file}\")\n",
        "print(f\"Calibration file: {cal_file}\")\n",
        "print(f\"Settings file: {settings_file}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0CFejMxlAjge"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "from configparser import ConfigParser\n",
        "import shutil\n",
        "current_dir = os.getcwd()\n",
        "log = logging.getLogger('netCDF')\n",
        "\n",
        "class BaseConverter(object):\n",
        "    \"\"\"Base class for csv file to netCDF PAD converters. This base class\n",
        "    handles the request and parsing of a settings file and system.ini\n",
        "    file.\n",
        "    \"\"\"\n",
        "    def __init__(self, get_settings=True):\n",
        "\n",
        "        # Instantiate attributes.\n",
        "        self.package_path = None\n",
        "        self.metadata = None\n",
        "        self.manifest = None\n",
        "\n",
        "        # Get settings file to use and grab DOI URL from file (may be Null if\n",
        "        # we'll be minting DOIs per dataset).\n",
        "        if get_settings:\n",
        "            # settings_file = self.get_settings_path()\n",
        "            with open(settings_file) as f:\n",
        "                settings = json.load(f)\n",
        "\n",
        "            self.citation = settings['citationText']\n",
        "            self.doi = settings['doi']\n",
        "            self.doi_minting = settings['mintDOI']\n",
        "            self.version = settings['version']\n",
        "\n",
        "            if 'acknowledgement' in settings:\n",
        "                self.acknowledgement = settings['acknowledgement']\n",
        "\n",
        "        # Get root directory of CPI system.\n",
        "        cwd = os.getcwd()\n",
        "        cpi_root = os.path.join(cwd.split('/CPI')[0], 'CPI')\n",
        "\n",
        "        # Read system.ini file\n",
        "        system_file = os.path.join(cpi_root, 'system.ini')\n",
        "        system = ConfigParser()\n",
        "        system.read(system_file)\n",
        "\n",
        "        # Setting paths\n",
        "        self.starting_path = os.getcwd()\n",
        "        self.log_base = os.path.join(os.getcwd(), 'logBasePath')\n",
        "        self.log_level = os.path.join(os.getcwd(), 'logLevel')\n",
        "\n",
        "        # Check if the log directory exists, if not, create it\n",
        "        if not os.path.exists(self.log_base):\n",
        "            os.makedirs(self.log_base)  # Create the directory if it doesn't exist\n",
        "            print(f\"Created log base directory: {self.log_base}\")\n",
        "\n",
        "        # Check if the log level file exists, create it if it doesn't\n",
        "        if not os.path.exists(self.log_level):\n",
        "            with open(self.log_level, 'w') as f:\n",
        "                f.write('INFO')  # You can change the log level to your preference\n",
        "            print(f\"Created log level file: {self.log_level}\")\n",
        "\n",
        "        # Setup logging configuration\n",
        "        self.setup_logging()\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Setup logging to write to a file in the log base path.\"\"\"\n",
        "        log_level = logging.INFO  # Default to INFO if file is not available\n",
        "\n",
        "        # Check the log level from the file, default to INFO if file not found or invalid content\n",
        "        if os.path.exists(self.log_level):\n",
        "            with open(self.log_level, 'r') as f:\n",
        "                level = f.read().strip()\n",
        "                # Map the string level to logging constants, default to INFO if invalid\n",
        "                log_level = getattr(logging, level, logging.INFO)\n",
        "\n",
        "        log.setLevel(log_level)\n",
        "\n",
        "        # Set log file path\n",
        "        log_file_path = os.path.join(self.log_base, 'app.log')\n",
        "\n",
        "        # Create file handler for logging\n",
        "        file_handler = logging.FileHandler(log_file_path)\n",
        "        file_handler.setLevel(log_level)\n",
        "\n",
        "        # Create formatter for log messages\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        file_handler.setFormatter(formatter)\n",
        "\n",
        "        # Add file handler to logger\n",
        "        log.addHandler(file_handler)\n",
        "\n",
        "        # You can also add a stream handler for console output if needed\n",
        "        stream_handler = logging.StreamHandler()\n",
        "        stream_handler.setLevel(log_level)\n",
        "        stream_handler.setFormatter(formatter)\n",
        "        log.addHandler(stream_handler)\n",
        "\n",
        "\n",
        "        print(f\"Logging is set up. Logs will be written to {log_file_path}\")\n",
        "\n",
        "\n",
        "    def get_settings_path(self):\n",
        "        \"\"\"Get path to settings file\n",
        "\n",
        "        The settings file is a json file containing control parameters.\n",
        "        For netCDF conversion this is used to pass in existing DOI\n",
        "        information only despite the settings file (also used for\n",
        "        pre-ingest)  contains many more values.\n",
        "\n",
        "        This method asks the user for the name of the settings file to use.\n",
        "        Settings files are located in a folder named cpi_settings/pad in the\n",
        "        user's home directory. The important key/value pairs for this method is\n",
        "        \"doi\": \"https://doi.org/10.25921/saca-sp25\" \"citation\": \"Citation text\".\n",
        "\n",
        "        Returns:\n",
        "            settings_path(str): path to the settings json file.\n",
        "\n",
        "        \"\"\"\n",
        "        # Get list of settings files in user home directory\n",
        "        root_path = os.path.join(os.path.expanduser(\"~\"),\n",
        "                                 'cpi_settings/pad')\n",
        "        root, dirs, files = self.walk_dir(root_path)\n",
        "        files.sort()\n",
        "        print(f'\\nSettings files in {root}:  {files}')\n",
        "        # settings_file = input(\"Enter name of settings file to use: \")\n",
        "        settings_path = 'settings.json'\n",
        "        # if not os.path.splitext(settings_file)[1]:\n",
        "        #     settings_file = settings_file + '.json'\n",
        "        # settings_path = os.path.join(root, settings_file)\n",
        "        if os.path.isfile(settings_path):\n",
        "            return settings_path\n",
        "        else:\n",
        "            print(f'{settings_path} is not a valid file path. '\n",
        "                  f'Enter name of the file again: ')\n",
        "            return self.get_settings_path()\n",
        "\n",
        "    def get_metadata(self, metadata_file):\n",
        "        \"\"\"Open metadata file and convert to Python dictionary held in\n",
        "        self.metadata attribute.\n",
        "\n",
        "        Args:\n",
        "            metadata_file (str): Full path to metadata file\n",
        "\n",
        "        \"\"\"\n",
        "        with open(metadata_file, 'r') as f:\n",
        "            self.metadata = json.loads(f.read())\n",
        "\n",
        "        self.metadata['DOI'] = self.doi\n",
        "\n",
        "    def parse_manifest(self, manifest_path, backup=True):\n",
        "        \"\"\"Parse manifest file and convert into a dictionary with file path and\n",
        "        key and checksum as value and store in attribute self.manifest.\n",
        "\n",
        "        Args:\n",
        "            manifest_path (str): Full path to manifest file.\n",
        "            backup (boo): Control whether to make copy of manifest file before\n",
        "                          parsing.\n",
        "\n",
        "        \"\"\"\n",
        "        if backup:\n",
        "            # Make a backup of the original manifest before continuing. If a\n",
        "            # backup already exists skip overwriting to preserve original\n",
        "            # manifest when doing multiple processing runs.\n",
        "            path_parts = os.path.split(manifest_path)\n",
        "            backup_path = os.path.join(path_parts[0],\n",
        "                                       f'original-{path_parts[1]}')\n",
        "            if not os.path.isfile(backup_path):\n",
        "                shutil.copy2(manifest_path, backup_path)\n",
        "\n",
        "        manifest = {}\n",
        "        with open(manifest_path) as m:\n",
        "            for line in m:\n",
        "                entry = line.split('  ')\n",
        "                # print(entry[1][:-1], entry[0])\n",
        "                manifest[entry[1][:-1]] = entry[0]\n",
        "\n",
        "        self.manifest = manifest\n",
        "\n",
        "    def mint_doi(self):\n",
        "        \"\"\"Mint DOI. This is a placeholder for eventual development.\n",
        "\n",
        "        \"\"\"\n",
        "        print(f'Code can not yet mint a dataset;level DOI')\n",
        "        log.critical(f'Code can not yet mint a dataset-level DOI')\n",
        "        self.metadata['DOI'] = ''\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q9B6gLr7Ajge"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Converter for generating data netCDF files from MANTA output .csv files.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import csv\n",
        "import logging\n",
        "import os.path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "from datetime import datetime, date\n",
        "from netCDF4 import date2num, num2date\n",
        "\n",
        "log = logging.getLogger('MANTA.Converter')\n",
        "\n",
        "\n",
        "class MantaConvert():\n",
        "\n",
        "    def __init__(self, file_path, metadata, cal_data, version):\n",
        "        super(MantaConvert, self).__init__()\n",
        "\n",
        "        self.converter_version = 'v.1.2.0'\n",
        "        self.ds = None\n",
        "        self.out_file = None\n",
        "        self.manta_nc = None\n",
        "        self.metadata = metadata\n",
        "        self.file_path = file_path\n",
        "        self.cal_data = cal_data\n",
        "        self.version = version\n",
        "        self.old_file = None\n",
        "\n",
        "\n",
        "        # Check that the data file exists.\n",
        "        # if not os.path.isfile(file_path):\n",
        "        #     raise IOError(f'File {file_path} is not in data package.')\n",
        "\n",
        "        clean_data = self.process_csv()\n",
        "\n",
        "        # Get timestamps from clean_data DataFrame.\n",
        "        raw_time_stamps = clean_data['Timestamp']\n",
        "        self.time_stamps = np.array([stamp.to_pydatetime() for stamp in raw_time_stamps[1:]])\n",
        "        raw_freqs = clean_data.columns.values.tolist()\n",
        "        self.frequencies = np.array([float(x) for x in raw_freqs[2:]])\n",
        "\n",
        "\n",
        "        time_stamps = np.array([stamp.to_pydatetime() for stamp in\n",
        "                                raw_time_stamps[1:]])\n",
        "        # Get frequencies from DataFrame column values.\n",
        "\n",
        "        frequencies = np.array([float(x) for x in raw_freqs[2:]])\n",
        "\n",
        "        # Get the effort seconds for the data rows.\n",
        "        efforts = clean_data['0'].values[1:]\n",
        "        self.efforts = efforts\n",
        "\n",
        "        # Get main data array from DataFrame.\n",
        "        psd = clean_data.iloc[1:, 2:].to_numpy()\n",
        "        self.psd = psd\n",
        "\n",
        "        # Create nc file.\n",
        "        self.create_acoustic_nc(time_stamps, frequencies, efforts, psd,\n",
        "                                metadata, file_path)\n",
        "\n",
        "    def process_csv(self):\n",
        "        \"\"\"Read CSV file into a Pandas DataFrame and then clean data of\n",
        "        duplicate rows and rows with a duplicate timestamp but with an effort\n",
        "        of 1 second. These rows are a known errant output from MANTA.\n",
        "\n",
        "        Returns:\n",
        "            clean_data (pd.DataFrame): Cleaned DataFrame of .csv file data.\n",
        "            None - returned when there are duplicate rows that both appear to\n",
        "            have real data.\n",
        "        \"\"\"\n",
        "        parse_dates = {\"Timestamp\": [0]}\n",
        "        # Convert csv file to Pandas DataFrame\n",
        "        raw_data = pd.read_csv(self.file_path, parse_dates=parse_dates)\n",
        "\n",
        "\n",
        "        # Remove rows that are complete duplicates and log removal.\n",
        "        no_dups = raw_data.drop_duplicates()\n",
        "        if raw_data.shape != no_dups.shape:\n",
        "            removed_num = raw_data.shape[0] - no_dups.shape[0]\n",
        "            log.info(f'{removed_num} duplicate rows were removed.from'\n",
        "                     f' {os.path.basename(self.file_path)}')\n",
        "\n",
        "        # Sort by Timestamp and effort seconds (column \"0\" which is the column\n",
        "        # name not its index)) and check for duplicate timestamp values.\n",
        "        no_dups = no_dups.sort_values(by=[\"Timestamp\", \"0\"])\n",
        "        time_diffs = no_dups['Timestamp'].diff()\n",
        "        drop_rows = []\n",
        "        for index, value in enumerate(time_diffs):\n",
        "            value = value.total_seconds()\n",
        "            first_effort = no_dups.iloc[index - 1][1]\n",
        "            second_effort = no_dups.iloc[index][1]\n",
        "            if value == 0.0:\n",
        "                # We have found a set of duplicate timestamps. THere is a MANTA\n",
        "                # issue where a row with an effort of 1 second is followed by a\n",
        "                # row with a 60-second effort. The 1-second row is an error and\n",
        "                # needs to be dropped. If the effort is not 1, then there is a\n",
        "                # deeper issue so throw an error.\n",
        "                if first_effort == 1:\n",
        "                    # This is an error and this row should be dropped.\n",
        "                    drop_rows.append(index-1)\n",
        "                else:\n",
        "                    log.error(f'{os.path.basename(self.file_path)} has '\n",
        "                              f'duplicate timestamps with differing data '\n",
        "                              f'values and efforts over 1 second. '\n",
        "                              f'Skipping file.')\n",
        "                    raise ValueError\n",
        "            elif value == 59.0:\n",
        "                # This might be from a spurious 1-second effort row in\n",
        "                # addition to  full 60-second effort row for the same minute.\n",
        "                # Test for this and delete row if appropriate.\n",
        "                if second_effort == 1 and first_effort == 60:\n",
        "                    drop_rows.append(index)\n",
        "\n",
        "        if drop_rows:\n",
        "            log.info(f'The following row(s) qre 1-second errors and were '\n",
        "                     f'removed {drop_rows} from '\n",
        "                     f'{os.path.basename(self.file_path)}')\n",
        "            clean_data = no_dups.drop(drop_rows, axis=0)\n",
        "        else:\n",
        "            clean_data = no_dups\n",
        "\n",
        "        return clean_data\n",
        "\n",
        "    @staticmethod\n",
        "    def get_times(time_stamps, units='seconds since 1970-01-01T00:00:00Z'):\n",
        "        \"\"\"Get times from timestamps.\n",
        "\n",
        "        Args:\n",
        "            time_stamps (numpy array): Time stamp array.\n",
        "            units (str): units for time conversion. seconds by default.\n",
        "\n",
        "        Returns:\n",
        "            cmd (numpy array): Times array.\n",
        "        \"\"\"\n",
        "\n",
        "        cdm_time = []\n",
        "        for stamp in time_stamps:\n",
        "            temp_stamp = date2num(stamp, units=units)\n",
        "            cdm_time.append(temp_stamp)\n",
        "        return np.array(cdm_time)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_dates(times, units='seconds since 1970-01-01T00:00:00Z'):\n",
        "        \"\"\"Get times from timestamps.\n",
        "\n",
        "        Args:\n",
        "            times (list): list of integer seconds since timestamps\n",
        "            units (str): units for time conversion. seconds by default.\n",
        "\n",
        "        Returns:\n",
        "            dates (list): list of times converted to ISO timestamps.\n",
        "        \"\"\"\n",
        "\n",
        "        dates = []\n",
        "        for stamp in times:\n",
        "            temp_stamp = num2date(stamp, units=units)\n",
        "            dates.append(temp_stamp)\n",
        "\n",
        "        return dates\n",
        "\n",
        "    def get_manta_netcdf(self):\n",
        "        \"\"\"Get the MANTA netCDF_converters export file tor the data file\n",
        "        being processed.\n",
        "\n",
        "        Returns:\n",
        "            nc (netCDF_converters): Open netCDF_converters file.\n",
        "        \"\"\"\n",
        "        # Due to MANTA differences in file organization and data packaging\n",
        "        # there are several possible paths. We need to try them all to find\n",
        "        # the correct path.\n",
        "        path_1 = self.out_file.replace('.nc', '_netCDF.nc')\n",
        "        path_2 = path_1.replace('csv', 'netCDF')\n",
        "        path_3 = self.out_file.replace(f'_v{self.version}.nc',\n",
        "                                       f'_netCDF_v{self.version}.nc')\n",
        "        path_4 = path_3.replace('csv', 'netCDF')\n",
        "\n",
        "        paths = [path_1, path_2, path_3, path_4]\n",
        "        for file_path in paths:\n",
        "            if os.path.isfile(file_path):\n",
        "                with xr.open_dataset(file_path) as manta_ds:\n",
        "                    return manta_ds\n",
        "\n",
        "    @staticmethod\n",
        "    def time_variable_attrs(time_stamps, cdm_time):\n",
        "        \"\"\"Create time variables.\n",
        "\n",
        "        Args:\n",
        "            time_stamps (array): String timestamp values.\n",
        "            cdm_time (array):\n",
        "\n",
        "        Returns:\n",
        "            time_stamp_var: Formatted time stamp variable\n",
        "            time_var: Formatted time variable.\n",
        "\n",
        "        \"\"\"\n",
        "        time_stamp_var = {\n",
        "            'actual_range': (time_stamps[0].isoformat(),\n",
        "                             time_stamps[-1].isoformat()),\n",
        "            # 'comment': 'Start times of 1-minute bins over which '\n",
        "            #            'sound pressure levels are calculated',\n",
        "            'long_name': \"ISO timestamp beginning each 1-minute temporal \"\n",
        "                         \"bin\",\n",
        "            'standard_name': \"time\",\n",
        "            'units': 'seconds'\n",
        "        }\n",
        "\n",
        "        time_var = {\n",
        "            'actual_range': (cdm_time[0], cdm_time[-1]),\n",
        "            'long_name': \"Time beginning each 1-minute temporal bin\",\n",
        "            'standard_name': \"time\",\n",
        "            'units': \"seconds since 1970-01-01T00:00:00Z\",\n",
        "            'coverage_content_type': 'coordinate'\n",
        "        }\n",
        "\n",
        "        return time_stamp_var, time_var\n",
        "\n",
        "    @staticmethod\n",
        "    def static_metadata(md):\n",
        "        \"\"\"Static metadata information be be added ro metadata dictionary.\n",
        "\n",
        "        Returns:\n",
        "            static_metadata (dict): static metadata dictionary.\n",
        "        \"\"\"\n",
        "        static_metadata = {\n",
        "            'keywords': 'GCMD:oceans, GCMD:ocean acoustics, GCMD:ambient noise,'\n",
        "                        ' intensity, GCMD:marine environment monitoring, '\n",
        "                        'marine habitat, sound intensity level in water, '\n",
        "                        'soundscapes',\n",
        "            'keyVocabulary': \"GCMD: GCMD Keywords\",\n",
        "            'source': \"Data analysis was performed using the Making Ambient \"\n",
        "                      \"Noise Trends Accessible (MANTA, https://bitbucket.org\"\n",
        "                      \"/CLO-BRP/manta-wiki/wiki/Home, see Miksis-Olds et al., \"\n",
        "                      \"2021; Martin et al., 2021a,b) standalone software \"\n",
        "                      f\"({md['DATASET_DETAILS']['SOFTWARE_VERSION']}) \"\n",
        "                      f\"to produce hybrid millidecade spectra of \"\n",
        "                      \"sound levels from ocean audio recordings. To \"\n",
        "                      \"efficiently tackle large datasets, MANTA is designed \"\n",
        "                      \"around a parallel-processing Matlab package, Raven-X \"\n",
        "                      \"(Dugan et. al., 2014, 2016, and 2018) that uses \"\n",
        "                      \"ordinary multi-core computers to accelerate processing \"\n",
        "                      \"speeds. MANTA calculates the sound pressure spectral \"\n",
        "                      \"density (PSD) levels in units of 1 µPa^2/Hz using \"\n",
        "                      \"Welch's Method in Matlab. The Discrete Fourier \"\n",
        "                      \"Transform length is equal to the sample rate, a Hann \"\n",
        "                      \"window of equal length is applied to the data and 50% \"\n",
        "                      \"overlap is used. This results in PSD estimates of \"\n",
        "                      \"mean-square pressure amplitude (µPa^2) with a frequency \"\n",
        "                      \"resolution of 1 Hz and temporal resolution of 1 second. \"\n",
        "                      \"The 120 PSD estimates from each 1-minute segment were \"\n",
        "                      \"averaged, and the average spectrum for each minute was \"\n",
        "                      \"further processed to a hybrid millidecade (HMD) \"\n",
        "                      \"spectrum as dB re 1 µPa^2/Hz, as defined in Martin et \"\n",
        "                      \"al. (2021b). Hybrid millidecades are an efficient means \"\n",
        "                      \"of storing PSD spectra from high sample rate audio \"\n",
        "                      \"files using 1-Hz values up to 435 Hz, then millidecade \"\n",
        "                      \"wide PSD values up to one half of the sampling rate \"\n",
        "                      \"(Martin et al., 2021b). The MANTA outputs for each day \"\n",
        "                      \"are: (1) CSV of the 1 minute HMD results; (2) image of \"\n",
        "                      \"the daily long-term spectral average based on the \"\n",
        "                      \"1 minute HMD results, (3) image of the daily spectral \"\n",
        "                      \"probability density with percentiles, and (4) NetCDF \"\n",
        "                      \"containing products 2 and 3 in addition to a deployment-\"\n",
        "                      \"level MANTA Metadata output file containing the \"\n",
        "                      \"associated frequency-dependent calibration data used to \"\n",
        "                      \"compute the calibrated spectrum levels.\",\n",
        "            'references':  'Original audio recordings are available '\n",
        "                           'open-access: '\n",
        "                           'https://www.ncei.noaa.gov/maps'\n",
        "                           '/passive-acoustic-data/. Computation of '\n",
        "                           'single-sided mean-square sound pressure '\n",
        "                           'spectral density with 1 Hz resolution followed '\n",
        "                           'ISO 18405 3.1.3.13 (International Standard ISO 1'\n",
        "                           '8405:2017(E), Underwater Acoustics – Terminology. '\n",
        "                           'Geneva: ISO). Hybrid millidecade band processing '\n",
        "                           'followed Martin et al. (2021; '\n",
        "                           'https://doi.org/10.1121/10.0003324)',\n",
        "            'license': 'CC0-1.0',\n",
        "        }\n",
        "\n",
        "        return static_metadata\n",
        "\n",
        "    def get_metadata(self, md):\n",
        "        \"\"\"Update global netCDF_converters metadata information.\n",
        "\n",
        "        Args:\n",
        "            md (dict): Dictionary containing dataset metadata.\n",
        "\n",
        "        \"\"\"\n",
        "        creation_date = date.today().isoformat()\n",
        "        md.update(self.static_metadata(md))\n",
        "        data_quality, quality_bins = self.get_quality(md)\n",
        "\n",
        "        # Process raw scientists, projects and sponsors.\n",
        "        scientists = []\n",
        "        for entry in md['SCIENTISTS']:\n",
        "            scientists.append(entry['name'])\n",
        "        scientists = ', '.join(scientists)\n",
        "\n",
        "        project = ', '.join(md['PROJECT_NAME'])\n",
        "        # Hack to fix ONMS_sound project name\n",
        "        if project == 'ONMS':\n",
        "            project = 'ONMS Sound'\n",
        "\n",
        "        sources = []\n",
        "        for entry in md['SPONSORS']:\n",
        "            sources.append(entry['name'])\n",
        "        sources = ', '.join(sources)\n",
        "\n",
        "        raw_metadata = {\n",
        "            'acknowledgement': '',\n",
        "            'history': f'Original hybrid millidecade spectra were produced by '\n",
        "                       f'{scientists}. NCEI created this single '\n",
        "                       f'standards-compliant netCDF file from the MANTA '\n",
        "                       f'outputs plus additional metadata from the deployment '\n",
        "                       f'and overall project. Conversion was done using '\n",
        "                       f'{self.converter_version} of the NCEI MANTA netCDF '\n",
        "                       f'converter.',\n",
        "            'citation': f'Cite as: {md[\"citation\"]}',\n",
        "            'comment': data_quality,\n",
        "            'creator_name': scientists,\n",
        "            'creator_role': \"Principal Investigator\",\n",
        "            'conventions': 'COARDS, CF-1.6, ACDD-1.3',\n",
        "            'publisher_email': 'pad.info@noaa.gov',\n",
        "            'publisher_name': ' NOAA National Centers for Environmental '\n",
        "                               'Information',\n",
        "            'publisher_type': 'institution',\n",
        "            'publisher_url': 'https://www.ncei.noaa.gov/products'\n",
        "                             '/passive-acoustic-data',\n",
        "            'date_created': creation_date,\n",
        "            'id': md['DOI'],\n",
        "            'product_version': f'v{md[\"version\"]}',\n",
        "            'naming_authority': 'NOAA National Centers for Environmental '\n",
        "                                'Information',\n",
        "            'infoUrl': 'https://ncei.noaa.gov',\n",
        "            'institution': sources,\n",
        "            'geospatial_bounds': f\"POINT ({md['DEPLOYMENT']['DEPLOY_LAT']} \"\n",
        "                                 f\"{md['DEPLOYMENT']['DEPLOY_LON']})\",\n",
        "            'time_coverage_duration': \"P1D\",\n",
        "            'time_coverage_resolution': \"P60S\",\n",
        "            'keywords': md[\"keywords\"],\n",
        "            'keywords_vocabulary': md[\"keyVocabulary\"],\n",
        "            'license': md['license'],\n",
        "            'project': project,\n",
        "            'instrument': md['INSTRUMENT_TYPE'],\n",
        "            'standard_name_vocabulary': 'CF Standard Name Table v80',\n",
        "            'summary': md['ABSTRACT'],\n",
        "            'source': md['source'],\n",
        "            'title':  md['TITLE'],\n",
        "            'time_offset': f\"{md['DATASET_DETAILS']['ANALYSIS_TIME_ZONE']} \"\n",
        "                           f\"hours from UTC\",\n",
        "            'reference': md['references']\n",
        "        }\n",
        "\n",
        "\n",
        "        # Populate acknowledgement field with value from settings file or\n",
        "        # remove key from raw_metadata if there is no acknowledgement\n",
        "        # information.\n",
        "        if 'acknowledgement' in md:\n",
        "            if md['acknowledgement']:\n",
        "                raw_metadata['acknowledgement'] = md['acknowledgement']\n",
        "            else:\n",
        "                raw_metadata.pop('acknowledgement')\n",
        "        else:\n",
        "            raw_metadata.pop('acknowledgement')\n",
        "\n",
        "\n",
        "        keys = sorted(raw_metadata.keys())\n",
        "        global_attrs = {key:raw_metadata[key] for key in keys}\n",
        "\n",
        "\n",
        "        return global_attrs, quality_bins\n",
        "\n",
        "    def get_quality(self, raw_metadata, format='%Y-%m-%dT%H:%M:%S'):\n",
        "\n",
        "        \"\"\"Convert raw data quality information into JSON string suitable for\n",
        "        map viewer display.\n",
        "\n",
        "        Args:\n",
        "            raw_metadata(dict): Dictionary of dataset-level metadata.\n",
        "            format (str): Timestamp strong format.\n",
        "\n",
        "        Returns(dict): parsed quality details.\n",
        "\n",
        "        \"\"\"\n",
        "        quality_list = []\n",
        "        quality_bins = []\n",
        "\n",
        "        quality_values = {'Good': 1,\n",
        "                          'Unverified': 2,\n",
        "                          'Compromised': 3,\n",
        "                          'Unusable': 4}\n",
        "\n",
        "        raw_quality = raw_metadata['QUALITY_DETAILS']['quality_details']\n",
        "        for index, entry in enumerate(raw_quality):\n",
        "            if entry['quality'] == 'Select Quality Level':\n",
        "                # This is a bogus entry from PassivePacker so set to nothing.\n",
        "                continue\n",
        "            start = entry[\"start\"]\n",
        "            end = entry[\"end\"]\n",
        "            low_freq = entry['low_freq']\n",
        "            high_freq = entry['high_freq']\n",
        "            if low_freq and high_freq:\n",
        "                freq_str = f'from {low_freq}Hz to {high_freq}Hz'\n",
        "            else:\n",
        "                freq_str = f''\n",
        "\n",
        "            raw_channels = entry['channels']\n",
        "            if raw_channels:\n",
        "                if len(raw_channels) == 1:\n",
        "                    chan_str = f'for channel {raw_channels}'\n",
        "                else:\n",
        "                    chan_str = f'for channels {raw_channels}'\n",
        "            else:\n",
        "                chan_str = ''\n",
        "\n",
        "            quality = (f'Data quality: {entry[\"quality\"]} {start} to {end}'\n",
        "                       f' {chan_str} '\n",
        "                       f'{freq_str}')\n",
        "\n",
        "            details = ''\n",
        "            if 'comments' in entry:\n",
        "                details = entry['comments']\n",
        "            if details:\n",
        "                if len(details) > 4:\n",
        "                    # Check length of comments string. Weed out short comments\n",
        "                    # that are likely just line breaks or something else silly.\n",
        "                    quality = f'{quality}. {details.strip()}'\n",
        "\n",
        "            quality_list.append(quality)\n",
        "\n",
        "            # Convert numbers to \"seconds since\" integers for time indexing\n",
        "            # into quality_flag array.\n",
        "            start_stamp = datetime.strptime(start, format)\n",
        "            end_stamp = datetime.strptime(end, format)\n",
        "\n",
        "            cdm_start, cdm_end = self.get_times([start_stamp, end_stamp])\n",
        "\n",
        "            quality_bins.append({'start': cdm_start, 'end': cdm_end,\n",
        "                                 'low_freq': float(low_freq),\n",
        "                                 'high_freq': float(high_freq),\n",
        "                                 'quality': quality_values[entry[\"quality\"]]})\n",
        "\n",
        "        quality_statement = '; '.join(quality_list)\n",
        "\n",
        "        return quality_statement, quality_bins\n",
        "\n",
        "    @staticmethod\n",
        "    def build_quality(time_length, freq_length):\n",
        "        \"\"\"Build data quality_flag array and attributes\n",
        "\n",
        "        Args:\n",
        "            time_length (int): Length of time axis.\n",
        "            freq_length (int): Length of frequency axis.\n",
        "\n",
        "        Returns:\n",
        "            quality (mupy array): Initialized data quality array with default\n",
        "            values.\n",
        "            quality_attrs (dict)L Dictionary of data quality attributes for\n",
        "            creating xarray variable.\n",
        "        \"\"\"\n",
        "\n",
        "        quality_attrs = {\n",
        "            'long_name': 'Data quality flag',\n",
        "            'standard_name': 'quality_flag',\n",
        "            'comment': '1 = Good, 2 = Not evaluated/Unknown, '\n",
        "                       '3 =  Compromised/Questionable , 4 = Unusable / Bad',\n",
        "            'coverage_content_type': \"qualityInformation\"\n",
        "        }\n",
        "\n",
        "        quality = np.full((time_length, freq_length), fill_value=2,\n",
        "                          dtype=np.int8)\n",
        "\n",
        "        return quality, quality_attrs\n",
        "\n",
        "    def create_acoustic_nc(self, time_stamps, frequencies, efforts, psd,\n",
        "                           metadata, file_path):\n",
        "\n",
        "        time_stamps= self.time_stamps\n",
        "        frequencies=self.frequencies\n",
        "        efforts=self.efforts\n",
        "        psd=self.psd\n",
        "        metadata=self.metadata\n",
        "\n",
        "\n",
        "        # Set file path for new netCDF_converters and get Manta export netCDF_\n",
        "        # converters data an xarray.\n",
        "\n",
        "\n",
        "        filename = os.path.basename(file_path)\n",
        "        output_dir = os.path.join(current_dir, 'output')\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        self.out_file = os.path.join(output_dir, filename.replace('.csv', '.nc'))\n",
        "\n",
        "\n",
        "        # Try to get metadata from MANTA export file. This file is not always\n",
        "        # included so set manta_ds to None if that is the case.\n",
        "        try:\n",
        "            manta_ds = self.get_manta_netcdf()\n",
        "        except FileNotFoundError:\n",
        "            manta_ds = None\n",
        "            log.warning(f'MANTA metadata export file for {filename} not '\n",
        "                        f'found')\n",
        "\n",
        "        # Get times from csv_data time_stamp and get string timestamps.\n",
        "        cdm_time = self.get_times(time_stamps)\n",
        "        string_times = [t.isoformat() for t in time_stamps]\n",
        "\n",
        "        # Get time variable attribute dictionaries.\n",
        "        time_stamp_attrs, time_attrs = self.time_variable_attrs(time_stamps,\n",
        "                                                                cdm_time)\n",
        "\n",
        "        # Build frequency attribute dictionary.\n",
        "        freq_attrs = {'actual_range': (frequencies.min(), frequencies.max()),\n",
        "                      'long_name': 'Center frequency of hybrid millidecade '\n",
        "                                   'spectral bands',\n",
        "                      'standard_name': \"sound_frequency\",\n",
        "                      'units': \"Hz\",\n",
        "                      'coverage_content_type': 'coordinate'\n",
        "        }\n",
        "\n",
        "        # Build psd data attribute dictionary.\n",
        "        psd_attrs = {\n",
        "            'long_name': 'Single-sided mean-square sound pressure '\n",
        "                         'spectral density re 1 micropascal^2/Hz',\n",
        "            'standard_name': 'sound_intensity_in_water',\n",
        "            'units': 'dB',\n",
        "            'comment': 'Computation of single-sided mean-square sound '\n",
        "                       'pressure spectral density followed ISO 18405 '\n",
        "                       '3.1.3.13.',\n",
        "            'coverage_content_type': 'physicalMeasurement'\n",
        "        }\n",
        "\n",
        "        effort_attrs = {\n",
        "            'long_name': 'Duration of input data available for each 1-minute '\n",
        "                         'bin',\n",
        "            'units': 'seconds',\n",
        "            'coverage_content_type': \"qualityInformation\"\n",
        "        }\n",
        "\n",
        "        # Create coordinates entry.\n",
        "        coords = {'time': (['time'], cdm_time, time_attrs),\n",
        "                  'frequency': (['frequency'], frequencies, freq_attrs)}\n",
        "\n",
        "        # Get complete metadata entry.\n",
        "        full_metadata, quality_bins = self.get_metadata(metadata)\n",
        "\n",
        "        try:\n",
        "            full_metadata.update({\n",
        "                'PreampFixedGain_dB': manta_ds.PreampFixedGain_dB.item(0),\n",
        "                'SamplingRate': int(manta_ds.SamplingRate.item(0)),\n",
        "                'CalibrationFrequency_Hz': manta_ds.CalibrationFrequency_Hz.\n",
        "                item(0),\n",
        "                'CalibrationSensitivity_dB_re_1VperRefPress': manta_ds.\n",
        "                CalibrationSensitivity_dB_re_1VperRefPress.item(0),\n",
        "                'CalibrationDate': manta_ds.CalibrationDate.item(0).decode()\n",
        "            })\n",
        "        except Exception as e:\n",
        "            log.debug(f'Error \"{e}\" getting calibration data from MANTA '\n",
        "                      f'export netCDF '\n",
        "                      f'{self.out_file.replace(\".nc\", \"_netCDF.nc\")}')\n",
        "\n",
        "        # Build quality_flag data array and attributes\n",
        "        quality, quality_attrs = self.build_quality(cdm_time.shape[0],\n",
        "                                                    frequencies.shape[0])\n",
        "\n",
        "        # Build data variable dictionary for dataset.\n",
        "        data_vars = {'timestamp': (['time'], string_times, time_stamp_attrs),\n",
        "                     'effort': (['time'], efforts, effort_attrs),\n",
        "                     'psd': (['time', 'frequency'], psd, psd_attrs),\n",
        "                     'quality_flag': (['time', 'frequency'], quality,\n",
        "                                      quality_attrs)\n",
        "                     }\n",
        "\n",
        "        new_ds = xr.Dataset(data_vars=data_vars,\n",
        "                            coords=coords,\n",
        "                            attrs=full_metadata)\n",
        "\n",
        "        # Copy calibration data into new_ds.\n",
        "        new_ds['analog_sensitivity'] = self.cal_data['analog_sensitivity']\n",
        "        new_ds['preamp_gain'] = self.cal_data['preamp_gain']\n",
        "        new_ds['recorder_gain'] = self.cal_data['recorder_gain']\n",
        "        new_ds['sensor_sensitivity'] = self.cal_data['sensor_sensitivity']\n",
        "\n",
        "        # Set  custom _FillValue encoding to suppress the\n",
        "        # automatic _FillValue attribute xarray adds when writing to netCDF.\n",
        "        no_fill = {'_FillValue': None}\n",
        "        encoding = {var: no_fill for var in new_ds.data_vars}\n",
        "        encoding.update({var: no_fill for var in new_ds.coords})\n",
        "\n",
        "        # Update quality_flag values.\n",
        "        message = ''\n",
        "        errors = set()\n",
        "        for bin in quality_bins:\n",
        "            # Test time ranges to confirm quality range.\n",
        "            times = [bin['start'], bin['end'], new_ds.time[0],  new_ds.time[-1]]\n",
        "            quality_start, quality_end, time_start, time_end = self.get_dates(\n",
        "                                                                          times)\n",
        "            if time_end < quality_start or time_start > quality_end:\n",
        "                # This file's time range is completely outside the bin\n",
        "                # so skip to next bin.\n",
        "                continue\n",
        "            elif time_start < quality_start:\n",
        "                message = (f'{self.out_file} start {time_start} before quality '\n",
        "                           f'start {quality_start}')\n",
        "            if quality_end < time_end:\n",
        "                message = message + (f'{self.out_file} end {time_end} after '\n",
        "                                     f'quality end {quality_end}')\n",
        "\n",
        "            if message:\n",
        "                log.info(message)\n",
        "\n",
        "            try:\n",
        "                new_ds.quality_flag.loc[\n",
        "                              bin['start']: bin['end'],\n",
        "                              bin['low_freq']:bin['high_freq']] = bin['quality']\n",
        "            except Exception as e:\n",
        "                error_text = (f'Error \"{e}\" updating quality flag info for '\n",
        "                              f'{self.out_file}')\n",
        "                errors.update([error_text])\n",
        "\n",
        "        if errors:\n",
        "            for error in errors:\n",
        "                log.critical(error)\n",
        "\n",
        "        # If the version number is greater than 1 update the filename with\n",
        "        # the version number. Also check if a previous version is in the\n",
        "        # checksum manifest dictionary and remove it.\n",
        "        version = self.metadata['version']\n",
        "        if version > 1:\n",
        "            if os.path.isfile(self.out_file):\n",
        "                self.old_file = self.out_file\n",
        "\n",
        "            base_path = os.path.splitext(file_path)[0]\n",
        "            self.out_file = f'{base_path}_v{version}.nc'\n",
        "\n",
        "        # Write netCDF_converters file.\n",
        "        new_ds.to_netcdf(self.out_file, encoding=encoding)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "U1mGi7PPAjgg",
        "outputId": "bed015d3-890c-4a95-e8e7-5fda98806e6a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/SoundScape Conversion files/settings file/nrs-hmd.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6f9914b15752>\u001b[0m in \u001b[0;36m<cell line: 321>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m \u001b[0mMantaNetCDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-6f9914b15752>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \"\"\"\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-c6bcb8ee25c9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, get_settings)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mget_settings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# settings_file = self.get_settings_path()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0msettings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/SoundScape Conversion files/settings file/nrs-hmd.json'"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import logging\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# from manta_convert import MantaConvert\n",
        "# from common_tools import utilities as ut\n",
        "# from base_converter import BaseConverter\n",
        "\n",
        "log = logging.getLogger('MANTA')\n",
        "\n",
        "\n",
        "class MantaNetCDF(BaseConverter):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Class to automate the conversion product .csv files packaged by\n",
        "        PassivePacker into to netCDF_converters and add file to packages\n",
        "        manifest file.\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        # Get path to directory containing CSV files to process.\n",
        "        base_path = self.starting_path\n",
        "        directory = os.path.basename(base_path)\n",
        "\n",
        "\n",
        "        start = datetime.now()\n",
        "\n",
        "        # Get a list of potential package directories in the base path.\n",
        "        root, dirs, ignore = next(os.walk(base_path))\n",
        "\n",
        "        count = 0\n",
        "        errors = 0\n",
        "        for index, package in enumerate(dirs):\n",
        "\n",
        "            self.package_path = os.path.join(root, package)\n",
        "            # Get metadata.\n",
        "            try:\n",
        "\n",
        "                self.get_metadata(metadata_file)\n",
        "            except Exception as e:\n",
        "                if 'No such file or directory' in str(e):\n",
        "                    log.info(f'No metadata file in {package}, '\n",
        "                             f'This is likely not a data package')\n",
        "                else:\n",
        "                    log.critical(f'Error \"{e}\" getting metadata for {package}, '\n",
        "                                 f'Skipping package')\n",
        "                continue\n",
        "\n",
        "\n",
        "\n",
        "            # Look for json files in the data directory\n",
        "            data_dir = os.path.join(self.package_path )\n",
        "\n",
        "\n",
        "\n",
        "            # Update metadata with citation and version info from settings\n",
        "            # file parsed in base converter.\n",
        "            self.metadata['citation'] = self.citation\n",
        "            self.metadata['version'] = self.version\n",
        "            self.metadata['acknowledgement'] = self.acknowledgement\n",
        "\n",
        "            # Handle DOIs. Either use DOI passed in from settings file or\n",
        "            # mint a fresh DOI for this dataset.\n",
        "            if self.doi_minting:\n",
        "                self.mint_doi()\n",
        "            else:\n",
        "                self.metadata['DOI'] = self.doi\n",
        "\n",
        "\n",
        "            # Parse manifest file and make copy of the original if backup=True.\n",
        "            try:\n",
        "                manifest_file = os.path.join(self.package_path,\n",
        "                                             'manifest-md5.txt')\n",
        "                self.parse_manifest(manifest_file, backup=True)\n",
        "            except Exception as e:\n",
        "                pass\n",
        "                # log.critical(\n",
        "                #     f'Error \"{e}\" Parsing manifest for {package}, skipping '\n",
        "                #     f'package.')\n",
        "                # continue\n",
        "\n",
        "\n",
        "            # Get calibration and other metadata info from calibration file in\n",
        "            # calibration directory.\n",
        "            cal_metadata, raw_cal_data = self.parse_cal_file()\n",
        "\n",
        "            # Update calibration data xarray.\n",
        "            cal_data = self.update_cal_data(raw_cal_data)\n",
        "\n",
        "            # Get a master list of all files in all directories and\n",
        "            # subdirectories under data_files in package.\n",
        "            # data_dir = f'{self.package_path}/data'\n",
        "            raw_files = []\n",
        "            for entries in os.walk(data_dir):\n",
        "                files = [os.path.join(entries[0], file) for file in entries[2]\n",
        "                         if\n",
        "                         '._' not in file]\n",
        "                raw_files.extend(files)\n",
        "\n",
        "            # Process package\n",
        "            try:\n",
        "                self.process_package(cal_data)\n",
        "\n",
        "                # Create new manifest file with updated entries.\n",
        "                self.write_manifest(manifest_file)\n",
        "\n",
        "                log.info(f'Completed: {package}')\n",
        "                count += 1\n",
        "            except Exception as e:\n",
        "                log.critical(f'Error \"{e}\" processing {package}')\n",
        "                errors += 1\n",
        "\n",
        "\n",
        "        # Processing complete\n",
        "        total_time = datetime.now() - start\n",
        "        log.info(\n",
        "            f'Processing complete. {count} datasets processed in {total_time}')\n",
        "        if errors:\n",
        "            log.info(f'{errors} errors occurred during processing')\n",
        "\n",
        "    def process_package(self, cal_data):\n",
        "        \"\"\"Process the package to create netCDF_converters file and rearrange\n",
        "        files as needed.\n",
        "\n",
        "        Args:\n",
        "            cal_data (DataFrame): Pandas DataFrame containing calibration data.\n",
        "\n",
        "        \"\"\"\n",
        "        dir_path = self.package_path\n",
        "        manifest = self.manifest\n",
        "        metadata = self.metadata\n",
        "\n",
        "\n",
        "\n",
        "        csv_files = [csv_file]\n",
        "\n",
        "        for file in csv_files:\n",
        "\n",
        "            file_path = file\n",
        "\n",
        "\n",
        "            try:\n",
        "\n",
        "                converter = MantaConvert(file_path, metadata, cal_data,\n",
        "                                         self.version)\n",
        "                # converter.process_csv()\n",
        "                # converter.create_acoustic_nc()\n",
        "            except IOError as e:\n",
        "\n",
        "                log.critical(e)\n",
        "                continue\n",
        "            except ValueError:\n",
        "                # We've already logged the issue raising this error so just\n",
        "                # skip to next file.\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                log.critical(e)\n",
        "                continue\n",
        "            if not converter:\n",
        "                # We did not want to convert this file so pass.\n",
        "                log.error(f'No converter found for {file_path}')\n",
        "                continue\n",
        "\n",
        "            # Get checksum for new netCDF_converters file and add it to the\n",
        "            # manifest dictionary.\n",
        "            if converter.old_file:\n",
        "                # Remove old (previous version file) and pop its entry from\n",
        "                # the manifest dictionary.\n",
        "                os.remove(converter.old_file)\n",
        "                old_path = converter.old_file.replace(f'{dir_path}/', '')\n",
        "                if old_path in manifest:\n",
        "                    manifest.pop(old_path)\n",
        "            manifest_path = converter.out_file.replace(f'{dir_path}/', '')\n",
        "            # checksum = ut.get_checksum(converter.out_file)\n",
        "            # manifest[manifest_path] = checksum.split('  ')[0]\n",
        "            # log.debug(f'Created {manifest_path}')\n",
        "\n",
        "        # Rename all non-data netcdf files in package if this is a\n",
        "        # versioned dataset.\n",
        "        if self.version > 1:\n",
        "            self.rename_files()\n",
        "\n",
        "    def rename_files(self):\n",
        "        \"\"\"Programmatically append all filenames with _v<version number> for\n",
        "        datasets with a version greater than 1.\n",
        "\n",
        "        \"\"\"\n",
        "        manifest = self.manifest\n",
        "        new_manifest = {}\n",
        "        log.error(999999999999999999999999)\n",
        "        for key in manifest.keys():\n",
        "            this_path, file_name = os.path.split(key)\n",
        "            file_root, extension = os.path.splitext(file_name)\n",
        "            if not file_root[-2:] == f'v{self.version}':\n",
        "                new_name = f'{file_root}_v{self.version}{extension}'\n",
        "                log.debug(f'Renaming {file_name} to {new_name}')\n",
        "                new_key = os.path.join(this_path, new_name)\n",
        "                new_manifest[new_key] = manifest[key]\n",
        "                old_path = os.path.join(self.package_path, key)\n",
        "                if not os.path.isfile(old_path):\n",
        "                    # This file is in manifest but not in the data package.\n",
        "                    # Likely this is due to  a manifest thT was updated with\n",
        "                    # previous version files. Skip on to the next file.\n",
        "                    continue\n",
        "                new_path = os.path.join(self.package_path, new_key)\n",
        "                os.rename(old_path, new_path)\n",
        "            else:\n",
        "                # Pass Versioned files into new manifest file.\n",
        "                new_manifest[key] = manifest[key]\n",
        "\n",
        "        self.manifest = new_manifest\n",
        "\n",
        "    def write_manifest(self, manifest_path):\n",
        "        \"\"\"Write updated manifest path. WARNING! calling this method will\n",
        "        overwrite the original manifest file\n",
        "\n",
        "        Args:\n",
        "            manifest_path (str): Full path for output manifest file\n",
        "\n",
        "        \"\"\"\n",
        "        # with open(manifest_path, 'w') as m:\n",
        "        #     for path, checksum in self.manifest.items():\n",
        "        #         # Make sure path separators are unix style.\n",
        "        #         path.replace('\\\\', '/')\n",
        "        #         m.write(f'{checksum}  {path}\\n')\n",
        "\n",
        "\n",
        "    def walk_dir(self, cal_dir):\n",
        "        \"\"\"Walk through the calibration directory for files.\"\"\"\n",
        "        return next(os.walk(cal_dir), ([], [], []))\n",
        "\n",
        "    def parse_cal_file(self):\n",
        "        \"\"\"Get calibration and other metadata from calibration file.\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Get listing of files in calibration directory.\n",
        "        # cal_dir = os.path.join(self.package_path,  'calibration')\n",
        "        # if not os.path.isdir(cal_dir):\n",
        "        #     cal_dir = os.path.join(self.package_path,  'data_files')\n",
        "\n",
        "        # root, dirs, files = self.walk_dir(cal_dir)\n",
        "\n",
        "        # Filter out only Excel files from file list.\n",
        "        # files = [file for file in files if '.xls' in file and '._' not in file]\n",
        "        # files = [cal_file]\n",
        "\n",
        "        # Process calibration file.\n",
        "        # for file in files:\n",
        "        cal_metadata = pd.read_excel(cal_file, sheet_name=0)\n",
        "        cal_data = pd.read_excel(cal_file, sheet_name=1)\n",
        "        try:\n",
        "            cal_data.set_index('Frequency_Hz', inplace=True)\n",
        "            cal_data = cal_data.to_xarray()\n",
        "            return cal_metadata, cal_data\n",
        "        except Exception as e:\n",
        "            log.info(f'Calibration file {cal_file} had error \"{e}\". '\n",
        "\n",
        "                        f'Maybe not a proper calibration metadata file? ')\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def update_cal_data(self, raw_cal_data):\n",
        "        \"\"\"Update calibration data xarray Dataset. Changing writable names and\n",
        "        adding attributes\n",
        "\n",
        "        Args:\n",
        "            raw_cal_data (xarray): Raw calibration data xarray pulled from\n",
        "            calibration Excel file.\n",
        "\n",
        "        Returns:\n",
        "            cal_data (xarray): Updated calibration data xarray Dataset\n",
        "\n",
        "        \"\"\"\n",
        "        # Rename variables.\n",
        "        cal_data = raw_cal_data.rename(name_dict={\n",
        "                'Frequency_Hz': 'cal_frequency',\n",
        "                'AnalogSensitivity_dB_re_1VperRefPress': 'analog_sensitivity',\n",
        "                'PreampGain_dB': 'preamp_gain',\n",
        "                'RecorderGain_dB': 'recorder_gain',\n",
        "                'SensorSensitivity_dB_re_1V_perRefPress': 'sensor_sensitivity'})\n",
        "\n",
        "        # Update attributes.\n",
        "        cal_data.cal_frequency.attrs = {\n",
        "                        'long_name': 'Calibration frequency for hybrid '\n",
        "                                     'millidecade spectral bands',\n",
        "                        'units': 'Hz'}\n",
        "        cal_data.analog_sensitivity.attrs = {\n",
        "                            'long_name': 'Analog sensitivity re 1 V per '\n",
        "                                         'reference pressure',\n",
        "                            'units': 'dB',\n",
        "                            'coverage_content_type': 'physicalMeasurement',\n",
        "                            'comment': 'Sensitivity values in dB measured by '\n",
        "                                       'the manufacturer were linearly '\n",
        "                                       'interpolated to the center '\n",
        "                                       'frequencies of hybrid millidecade bands'\n",
        "        }\n",
        "        cal_data.sensor_sensitivity.attrs = {\n",
        "                            'long_name': 'Sensor sensitivity re 1 V per '\n",
        "                                         'reference pressure',\n",
        "                            'units': 'dB',\n",
        "                            'coverage_content_type': 'physicalMeasurement',\n",
        "                            'comment': 'Sensitivity values in dB measured by '\n",
        "                                       'the manufacturer were linearly '\n",
        "                                       'interpolated to the center '\n",
        "                                       'frequencies of HMD bands.'\n",
        "        }\n",
        "        cal_data.preamp_gain.attrs = {'long_name': 'Preamp gain', 'units': 'dB'}\n",
        "        cal_data.recorder_gain.attrs = {'long_name': 'Recorder gain',\n",
        "                                        'units': 'dB'}\n",
        "\n",
        "        return cal_data\n",
        "\n",
        "\n",
        "\n",
        "MantaNetCDF()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnmWoWIkAjgg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}